# KAROLINA-GPU
# Suggested modules
#
# ml OpenMPI/4.1.6-GCC-12.2.0-CUDA-12.4.0 PAPI/7.0.1-GCCcore-12.2.0
# ./build_libraries.sh karolina_cuda

# Karolina GPU has 2 x 64 cores
# 8x GPU accelerator NVIDIA A100 per node, 320GB HBM2 memory per node
# For good core placement, run with:
#  export OMP_PLACES=cores
#  export OMP_PROC_BIND=close
#  mpirun -n $SLURM_NTASKS --map-by ppr:$SLURM_NTASKS_PER_NODE:node:PE=$OMP_NUM_THREADS --bind-to core --report-bindings $EXE $CFG

#======== Vectorization ==========
#GPU Vlasov solvers no longer use any vectorclass.
#Options:
# AVX:	    VEC4D_AGNER, VEC4F_AGNER, VEC8F_AGNER
# AVX512:   VEC8D_AGNER, VEC16F_AGNER
# Fallback: VECTORCLASS = VEC_FALLBACK_GENERIC
VECTORCLASS = VEC_FALLBACK_GENERIC

# VMesh Block size: (historical value 4, also 8 supported)
#WID=8
WID=4

#======== Libraries ===========
LIBRARY_PREFIX = libraries-karolina_cuda

#======= Compiler and compilation flags =========
# NOTES on compiler flags:
# CXXFLAGS is for compiler flags, they are always used
# MATHFLAGS are for special math etc. flags, these are only applied on solver functions
# LDFLAGS flags for linker
# Important note: Do not edit COMPFLAGS in this file!

#-DNO_WRITE_AT_ALL:  Define to disable write at all to
#                    avoid memleak (much slower IO)
#-DMPICH_IGNORE_CXX_SEEK: Ignores some multiple definition
#                         errors that come up when using
#                         mpi.h in c++ on Cray

USE_CUDA=1

#-ggdb not available on nvcc
#-G (device debug) overrides --generate-line-info -line-info but also requires more device-side resources to run
# use "-Xptxas -v" for verbose output of ptx compilation
# --cudart shared used for Kostis' mempool_ts
CXXFLAGS = -g -O3 -x cu -std=c++17 --extended-lambda --expt-relaxed-constexpr -gencode arch=compute_80,code=sm_80 --cudart shared --generate-line-info -line-info -Xcompiler="-fopenmp" -Xcompiler="-fpermissive" -maxrregcount 24 -Wno-deprecated-declarations
testpackage: CXXFLAGS = -g -O2 -x cu -std=c++17 --extended-lambda --expt-relaxed-constexpr -gencode arch=compute_80,code=sm_80 --cudart shared --generate-line-info -line-info -Xcompiler="-fopenmp" -Xcompiler="-fpermissive" -maxrregcount 24 -Wno-deprecated-declarations

# Tell mpic++ to use nvcc for all compiling
CMP = OMPI_CXX='nvcc' OMPI_CXXFLAGS='' mpic++

# Now tell also the linker to use nvcc. Contents of these were retrieved with "mpic++ --showme:link".
# Use this same linker command also for building and linking phiprof.
## The line below indeed uses OMPI_CXX, not OMPI_LD
LNK = OMPI_CXX='nvcc' OMPI_CXXFLAGS='-arch=sm_80' OMPI_LIBS='-L/apps/all/OpenMPI/4.1.6-GCC-12.2.0-CUDA-12.4.0/lib -L/apps/all/hwloc/2.8.0-GCCcore-12.2.0/lib -L/apps/all/libevent/2.1.12-GCCcore-12.2.0/lib' OMPI_LDFLAGS='-Xlinker=-rpath=/apps/all/OpenMPI/4.1.6-GCC-12.2.0-CUDA-12.4.0/lib -Xlinker=-rpath=/apps/all/hwloc/2.8.0-GCCcore-12.2.0/lib -Xlinker=-rpath=/apps/all/libevent/2.1.12-GCCcore-12.2.0/lib -Xlinker=--enable-new-dtags -Xlinker=-rpath=$(CURDIR)/$(LIBRARY_PREFIX)/lib -lnvToolsExt -lgomp -lmpi_cxx -lmpi --cudart shared' mpic++

MATHFLAGS = --use_fast_math
# nvcc fast_math does not assume only finite math
testpackage: MATHFLAGS = --prec-sqrt=true --prec-div=true --ftz=false --fmad=false

#======== PAPI ==========
#Add PAPI_MEM define to use papi to report memory consumption?
CXXFLAGS += -DPAPI_MEM
testpackage: CXXFLAGS += -DPAPI_MEM

#======== Allocator =========
#NOTE: jemalloc not supported with GPUs

#======== Compiled Libraries ===========
INC_BOOST = -isystem $(LIBRARY_PREFIX)/include
LIB_BOOST = -L$(LIBRARY_PREFIX)/lib -lboost_program_options

INC_PAPI = -isystem $(LIBRARY_PREFIX)/include
LIB_PAPI = -L$(LIBRARY_PREFIX)/lib -lpapi

INC_ZOLTAN = -isystem /$(LIBRARY_PREFIX)/include
LIB_ZOLTAN = -L$(LIBRARY_PREFIX)/lib -lzoltan

INC_VLSV = -I$(LIBRARY_PREFIX)/include
LIB_VLSV = -L$(LIBRARY_PREFIX)/lib -lvlsv

INC_PROFILE = -I$(LIBRARY_PREFIX)/include
LIB_PROFILE = -L$(LIBRARY_PREFIX)/lib -lphiprof

#======== Header-only Libraries ===========
# included as submodules
